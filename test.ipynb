{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2022-10-19 15:23:52,669 utils.py:147] Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import importlib\n",
    "\n",
    "import py_vncorenlp\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import preprocess_img\n",
    "import LineOCR\n",
    "importlib.reload(LineOCR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import graph\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch_geometric\n",
    "importlib.reload(graph)\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import ChebConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\").type\n",
    "\n",
    "\n",
    "ID2LABEL_DICT = {\n",
    "    0: \"None\",\n",
    "    1: \"sign_date\",\n",
    "    2: \"diagnose\",\n",
    "    3:\"hospital_name\",\n",
    "    4:\"address\",\n",
    "    5:\"age\",\n",
    "    6:\"treatment\",\n",
    "    7:\"patient_name\",\n",
    "    8:\"admission_date\",\n",
    "    9:\"discharge_date\",\n",
    "    10:\"gender\",\n",
    "    11:\"document_type\",\n",
    "    12:\"department\",\n",
    "    13:\"note\",\n",
    "    14:\"BHYT\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PPYCode\\OCR\\ppy_paddle\n"
     ]
    }
   ],
   "source": [
    "#Load Text Segmentation engine\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=r'D:\\PPYCode\\OCR\\ppy_paddle\\weights\\nlp\\vncorenlp')\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\PPYCode\\\\OCR\\\\ppy_paddle'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'ppocr/postprocess/pse_postprocess/pse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PPYCode\\OCR\\ppy_paddle\\test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PPYCode/OCR/ppy_paddle/test.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Load all models\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PPYCode/OCR/ppy_paddle/test.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m preprocessImage \u001b[39m=\u001b[39m preprocess_img\u001b[39m.\u001b[39mPreprocessImage()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PPYCode/OCR/ppy_paddle/test.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lineDetAndOCR \u001b[39m=\u001b[39m LineOCR\u001b[39m.\u001b[39;49mProcessImage()\n",
      "File \u001b[1;32md:\\PPYCode\\OCR\\ppy_paddle\\LineOCR.py:30\u001b[0m, in \u001b[0;36mProcessImage.__init__\u001b[1;34m(self, detection_model_path, text_recognition_model_config_file, text_recognition_model_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m     26\u001b[0m              detection_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./PaddleOCR/pretrained_models/exported_det_model_221011/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m              text_recognition_model_config_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./default_vgg_seq2seq_config.yml\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m              text_recognition_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./weights/ocr/line_ocr_220930_3.pth\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[39m# self.line_detector = PaddleOCR(use_angle_cls=True, lang='en', det_model_dir = detection_model_path, use_gpu = True, det_algorithm=\"PSE\")\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_detector \u001b[39m=\u001b[39m PredLineDet\u001b[39m.\u001b[39;49mLineDetInfer(det_model_dir \u001b[39m=\u001b[39;49m detection_model_path)\n\u001b[0;32m     32\u001b[0m     \u001b[39m# config = Cfg.load_config_from_file(text_recognition_model_config_file)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     config \u001b[39m=\u001b[39m Cfg\u001b[39m.\u001b[39mload_config_from_file(text_recognition_model_config_file)\n",
      "File \u001b[1;32md:\\PPYCode\\OCR\\ppy_paddle\\PredLineDet.py:19\u001b[0m, in \u001b[0;36mLineDetInfer.__init__\u001b[1;34m(self, det_model_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdet_model_dir \u001b[39m=\u001b[39m det_model_dir\n\u001b[0;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdet_algorithm \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPSE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_detector \u001b[39m=\u001b[39m predict_det\u001b[39m.\u001b[39;49mTextDetector(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n",
      "File \u001b[1;32md:\\PPYCode\\OCR\\ppy_paddle\\PaddleOCR\\tools\\infer\\predict_det.py:135\u001b[0m, in \u001b[0;36mTextDetector.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    132\u001b[0m     sys\u001b[39m.\u001b[39mexit(\u001b[39m0\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_op \u001b[39m=\u001b[39m create_operators(pre_process_list)\n\u001b[1;32m--> 135\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess_op \u001b[39m=\u001b[39m build_post_process(postprocess_params)\n\u001b[0;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_tensor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_tensors, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m utility\u001b[39m.\u001b[39mcreate_predictor(\n\u001b[0;32m    137\u001b[0m     args, \u001b[39m'\u001b[39m\u001b[39mdet\u001b[39m\u001b[39m'\u001b[39m, logger)\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_onnx:\n",
      "File \u001b[1;32mc:\\Users\\luong\\anaconda3\\lib\\site-packages\\paddleocr\\ppocr\\postprocess\\__init__.py:55\u001b[0m, in \u001b[0;36mbuild_post_process\u001b[1;34m(config, global_config)\u001b[0m\n\u001b[0;32m     41\u001b[0m support_dict \u001b[39m=\u001b[39m [\n\u001b[0;32m     42\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mDBPostProcess\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEASTPostProcess\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSASTPostProcess\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFCEPostProcess\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     43\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCTCLabelDecode\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAttnLabelDecode\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mClsPostProcess\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSRNLabelDecode\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mVLLabelDecode\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPicoDetPostProcess\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     52\u001b[0m ]\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPSEPostProcess\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpse_postprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m PSEPostProcess\n\u001b[0;32m     56\u001b[0m     support_dict\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mPSEPostProcess\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[1;32mc:\\Users\\luong\\anaconda3\\lib\\site-packages\\paddleocr\\ppocr\\postprocess\\pse_postprocess\\__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpse_postprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m PSEPostProcess\n",
      "File \u001b[1;32mc:\\Users\\luong\\anaconda3\\lib\\site-packages\\paddleocr\\ppocr\\postprocess\\pse_postprocess\\pse_postprocess.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpaddle\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpaddle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mppocr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpostprocess\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpse_postprocess\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpse\u001b[39;00m \u001b[39mimport\u001b[39;00m pse\n\u001b[0;32m     31\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPSEPostProcess\u001b[39;00m(\u001b[39mobject\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m    The post process for PSE.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luong\\anaconda3\\lib\\site-packages\\paddleocr\\ppocr\\postprocess\\pse_postprocess\\pse\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m python_path \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexecutable\n\u001b[0;32m     20\u001b[0m ori_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd()\n\u001b[1;32m---> 21\u001b[0m os\u001b[39m.\u001b[39;49mchdir(\u001b[39m'\u001b[39;49m\u001b[39mppocr/postprocess/pse_postprocess/pse\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m subprocess\u001b[39m.\u001b[39mcall(\n\u001b[0;32m     23\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m setup.py build_ext --inplace\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(python_path), shell\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     25\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mCannot compile pse: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, if your system is windows, you need to install all the default components of `desktop development using C++` in visual studio 2019+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m     26\u001b[0m         \u001b[39mformat\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrealpath(\u001b[39m__file__\u001b[39m))))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'ppocr/postprocess/pse_postprocess/pse'"
     ]
    }
   ],
   "source": [
    "#Load all models\n",
    "preprocessImage = preprocess_img.PreprocessImage()\n",
    "lineDetAndOCR = LineOCR.ProcessImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread(r\"imgs_test\\705.jpeg\")\n",
    "test_img = preprocessImage(test_img)\n",
    "# lines, t = lineDetInfer.line_det_infer(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = test_img.copy()\n",
    "# for line in lines:\n",
    "#     pts = line.astype(int)\n",
    "#     pts = pts.reshape((-1, 1, 2))\n",
    "    \n",
    "#     isClosed = True\n",
    "    \n",
    "#     # Blue color in BGR\n",
    "#     color = (255, 0, 0)\n",
    "    \n",
    "#     # Line thickness of 2 px\n",
    "#     thickness = 2\n",
    "    \n",
    "#     image = cv2.polylines(image, [pts],\n",
    "#                         isClosed, color, thickness)\n",
    "\n",
    "# display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "result = process_engine.begin_recognize_text(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(len(result[\"croped_pil_img\"])):\n",
    "    line_img = result[\"croped_pil_img\"][iter]\n",
    "    text = result[\"text\"][iter]\n",
    "    ocr_score = result[\"ocr_score\"][iter]\n",
    "    display(line_img)\n",
    "    print(f\"{text} \\t {ocr_score}\")\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_xmin = []\n",
    "list_ymin = []\n",
    "list_xmax = []\n",
    "list_ymax = []\n",
    "list_Object = []\n",
    "\n",
    "for iter in range(len(result[\"bbox\"])):\n",
    "    xmin, ymin, xmax, ymax = result[\"bbox\"][iter]\n",
    "    text = result[\"text\"][iter]\n",
    "    list_xmin.append(xmin)\n",
    "    list_xmax.append(xmax)\n",
    "    list_ymin.append(ymin)\n",
    "    list_ymax.append(ymax)\n",
    "    list_Object.append(text)\n",
    "single_df = {\n",
    "    \"xmin\": list_xmin,\n",
    "    \"xmax\": list_xmax,\n",
    "    \"ymin\": list_ymin,\n",
    "    \"ymax\": list_ymax,\n",
    "    \"Object\": list_Object,\n",
    "    \"labels\": [\"None\"]*len(list_Object)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_df = pd.DataFrame.from_dict(single_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_df.to_excel(\"temp_data/temp_gcn/csv/temp.xlsx\")\n",
    "cv2.imwrite(\"temp_data/temp_gcn/img/temp.jpg\", test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_features(sentence):\n",
    "    segmented_sentence = rdrsegmenter.word_segment(sentence)\n",
    "    tokenized_sentence = torch.tensor([tokenizer.encode(segmented_sentence[0])])\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        features = phobert(tokenized_sentence)\n",
    "    return features\n",
    "\n",
    "def make_sent_bert_features(text):\n",
    "    features = get_sentence_features(text)\n",
    "    return features[1][0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'temp'\n",
    "connect = graph.Grapher(file)\n",
    "G,result, df = connect.graph_formation(export_graph=False)\n",
    "df = connect.relative_distance(export_document_graph = False)\n",
    "individual_data = from_networkx(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "            \"rd_b\",\n",
    "            \"rd_r\",\n",
    "            \"rd_t\",\n",
    "            \"rd_l\",\n",
    "            \"line_number\",\n",
    "            \"n_upper\",\n",
    "            \"n_alpha\",\n",
    "            \"n_spaces\",\n",
    "            \"n_numeric\",\n",
    "            \"n_special\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inference_data(df, individual_data):\n",
    "        text_features = []\n",
    "        for _, row in df.iterrows():\n",
    "            text_features.append(make_sent_bert_features(row[\"Object\"]))\n",
    "        text_features = np.asarray(text_features, dtype=np.float32)\n",
    "\n",
    "        numeric_features = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "        features = np.concatenate((numeric_features, text_features), axis=1)\n",
    "        features = torch.tensor(features)\n",
    "\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].str.strip()\n",
    "            except AttributeError as e:\n",
    "                pass\n",
    "        text = df[\"Object\"].values\n",
    "        individual_data.x = features\n",
    "\n",
    "        individual_data.text = text\n",
    "\n",
    "        test_list_of_graphs = []\n",
    "        test_list_of_graphs.append(individual_data)\n",
    "\n",
    "        test_one_data = \"\"\n",
    "        test_one_data = torch_geometric.data.Batch.from_data_list(test_list_of_graphs)\n",
    "        test_one_data.edge_attr = None\n",
    "        return test_one_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(edge_index=[2, 84], num_nodes=33, x=[33, 778], text=[1], batch=[33], ptr=[2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = preprocess_inference_data(df, individual_data)\n",
    "test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvoiceGCN(nn.Module):\n",
    "    def __init__(self, input_dim, chebnet=False, n_classes=5, dropout_rate=0.2, K=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        if chebnet:\n",
    "            self.conv1 = ChebConv(self.input_dim, 64, K=K)\n",
    "            # self.conv2 = ChebConv(64, 32, K=K)\n",
    "            self.conv3 = ChebConv(64, 64, K=K)\n",
    "            self.conv4 = ChebConv(64, self.n_classes, K=K)\n",
    "        else:\n",
    "            self.conv1 = GCNConv(self.first_dim, 64, improved=True, cached=True)\n",
    "            self.conv2 = GCNConv(64, 32, improved=True, cached=True)\n",
    "            self.conv3 = GCNConv(32, 16, improved=True, cached=True)\n",
    "            self.conv4 = GCNConv(16, self.n_classes, improved=True, cached=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # for transductive setting with full-batch update\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = F.dropout(F.relu(self.conv1(x, edge_index, edge_weight)), p=self.dropout_rate, training=self.training)\n",
    "        # x = F.dropout(F.relu(self.conv2(x, edge_index, edge_weight)), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.conv3(x, edge_index, edge_weight)), p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv4(x, edge_index, edge_weight)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceGCN(\n",
       "  (conv1): ChebConv(778, 64, K=3, normalization=sym)\n",
       "  (conv3): ChebConv(64, 64, K=3, normalization=sym)\n",
       "  (conv4): ChebConv(64, 15, K=3, normalization=sym)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(r\"D:\\PPYCode\\OCR\\ppy_paddle\\weights\\gcn\\GCN_221017.pth\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model(test_data).max(dim=1)[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SỞ Y TẾ THANH HÓA\tNone\n",
      "-----------------\n",
      "CÔNG HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM\tNone\n",
      "-----------------\n",
      "MS:01/BV-01\tNone\n",
      "-----------------\n",
      "BỆNH VIỆN ĐA KHOA TỈNH\thospital_name\n",
      "-----------------\n",
      "Độc lập - Tự do - Hạnh phúc\tNone\n",
      "-----------------\n",
      "Số lưu trữ: 2206586\tNone\n",
      "-----------------\n",
      "Khoa Nội Thận - Tiết Niệu\tdepartment\n",
      "-----------------\n",
      "Mã Y tế: 38/280/2022\tNone\n",
      "-----------------\n",
      "GIẤY RA VIỆN\tdocument_type\n",
      "-----------------\n",
      "- Họ tên người bệnh: NGUYỄN THỊ HỒNG MAI\tpatient_name\n",
      "-----------------\n",
      "Tuổi: 29\tage\n",
      "-----------------\n",
      "Giới tính: Nữ\tgender\n",
      "-----------------\n",
      "- Dân tộc: Kinh\tNone\n",
      "-----------------\n",
      "Nghề nghiệp: Khác\tNone\n",
      "-----------------\n",
      "- Mã số BHXH/Thẻ BHYT số:\tNone\n",
      "-----------------\n",
      "GD4383822386956\tBHYT\n",
      "-----------------\n",
      "- Địa chỉ: Nhân Trạch, Xã Hoằng Đạo, Huyện Hoằng Hóa, Tỉnh Thanh Hóa\taddress\n",
      "-----------------\n",
      "- Vào viện lúc: 20 giờ 10 phút, ngày 19 tháng 01 năm 2022\tadmission_date\n",
      "-----------------\n",
      "- Ra viện lúc: 15 giờ 00 phút, ngày 27 tháng 01 năm 2022\tdischarge_date\n",
      "-----------------\n",
      "- Chẩn đoán:\tNone\n",
      "-----------------\n",
      "+ Bệnh chính: N18.5 - Suy thận mạn, giai đoạn 5\tdiagnose\n",
      "-----------------\n",
      "khác; R64 - Suy mòn\tdiagnose\n",
      "-----------------\n",
      "+ Bệnh kèm theo: A41 - Nhiễm trùng khác,D64 - Các thiếu máu khác; 882 - Các thuyên tắc và huyết khối tỉnh mạch\tdiagnose\n",
      "-----------------\n",
      "+ Mô tả chi tiết:\tNone\n",
      "-----------------\n",
      "- Phương pháp điều trị: Truyền dịch - Lợi tiểu - hạ áp\ttreatment\n",
      "-----------------\n",
      "- Ghi chú: Ra viện uống thuốc theo đơn đã làm xết nghiêm SáRS-Cov-2 âm tính ngày 20/1-23/1-24/1-26/1\tnote\n",
      "-----------------\n",
      "TL.THỔ TRƯỞNG ĐƠN VỊ\tNone\n",
      "-----------------\n",
      "Ngày 27 tháng 01 năm 2022\tsign_date\n",
      "-----------------\n",
      "Ngày 27 tháng 01 năm 2022\tsign_date\n",
      "-----------------\n",
      "TRƯỞNG KHOA\tNone\n",
      "-----------------\n",
      "Ký tên đóng dâu)\tNone\n",
      "-----------------\n",
      "Ths.BS: Đào Thị Nga\tNone\n",
      "-----------------\n",
      "Ths.BS. Đào Thị Nga\tNone\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for idx, each in enumerate(y_preds):\n",
    "    text = test_data.text[0][idx]\n",
    "    pred_class = ID2LABEL_DICT[each]\n",
    "    print(text + '\\t' + pred_class)\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc5644fd2b52d511b211cf140a1e1d228fa8006a12c8d99adabf32e3d6df441d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
